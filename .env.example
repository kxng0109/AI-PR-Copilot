# AI Provider Selection
# Options: openai, anthropic, gemini, ollama
PRCOPILOT_AI_PROVIDER=openai

# Optional fallback provider if primary fails.
# Feel free to comment out if fallback is not needed
# Options: openai, anthropic, gemini, ollama, or just leave empty
PRCOPILOT_AI_FALLBACK_PROVIDER=

# Enable automatic fallback (true/false)
PRCOPILOT_AI_AUTO_FALLBACK=false

# AI Generation Settings (Applied to all providers)
AI_TEMPERATURE=0.1
AI_MAX_TOKENS=1024
AI_TIMEOUT_MILLIS=30000

# OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_BASE_URL=https://api.openai.com
OPENAI_MODEL=gpt-4o-mini

# Anthropic Claude Configuration
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
ANTHROPIC_BASE_URL=https://api.anthropic.com
ANTHROPIC_MODEL=claude-sonnet-4-0

# Google Gemini (Vertex AI) Configuration
# Get your project ID from Google Cloud Console
GEMINI_PROJECT_ID=your-gcp-project-id
GEMINI_LOCATION=us-central1
GEMINI_MODEL=gemini-2.0-flash

# Ollama Configuration (Local Models)
# Ensure Ollama is running: docker run -d -p 11434:11434 ollama/ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen3:4b

# PR Copilot Analysis Settings
PRCOPILOT_ANALYSIS_MAX_DIFF_CHARS=50000
PRCOPILOT_ANALYSIS_DEFAULT_LANGUAGE=en
PRCOPILOT_ANALYSIS_DEFAULT_STYLE=conventional-commits
PRCOPILOT_ANALYSIS_INCLUDE_RAW_MODEL_OUTPUT=false

# Logging (Debug Mode)
PRCOPILOT_LOG_PROMPTS=false
PRCOPILOT_LOG_RESPONSES=false